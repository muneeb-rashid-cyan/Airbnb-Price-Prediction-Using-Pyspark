{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Prediction model using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming I am working with Terabytes of Data, Pandas cannot handle such a large dataset due to lack of multiprocessing support, memory and speed.\n",
    "\n",
    "It is a tough decision to choose in between Dask and PySpark. However, In addition to other differences, PySpark is an all-in-one ecosystem which can handle the aggressive requirements with its MLlib, Structured data processing API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Libraries\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (OneHotEncoderEstimator, StringIndexer,\n",
    "                                VectorAssembler)\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.pipeline import Estimator, Transformer\n",
    "from pyspark.ml.regression import GBTRegressionModel, GBTRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, skewness\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print (\"Successfully imported Libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.43.160:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pricing_model</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112b84a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pricing_model').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a situation where we are working with big data, the connection will be to S3 bucket or cloud storage, depending on the storage infrasturucture we want decide to use.\n",
    "\n",
    "In this case, i will read the data from my local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'listings_processed.parquet'\n",
    "\n",
    "def import_data(DATA_PATH):\n",
    "    raw_data = spark.read.parquet(DATA_PATH)\n",
    "    return raw_data\n",
    "raw_data = import_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skewness\n",
    "In statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. in other words, it is the degree of distortion from the normal distribution. The skewness value can be positive or negative, or undefined. Unsurprisingly, our data is positvely skewed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   skewness(price)|\n",
      "+------------------+\n",
      "|23.313894822887782|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select(skewness('price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics\n",
    "This is just to help us get the distribution and iunderstanding of our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             price|\n",
      "+-------+------------------+\n",
      "|  count|             20114|\n",
      "|   mean|146.12210400715918|\n",
      "| stddev|120.79440354929586|\n",
      "|    min|               0.0|\n",
      "|    max|            8000.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select('price').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|summary|      square_feet|             price|          bedrooms|         bathrooms|      cleaning_fee|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
      "|  count|              423|             20114|             20107|             20105|             16490|\n",
      "|   mean| 514.113475177305|146.12210400715918|1.4492962649823444|1.1294951504600845| 38.91455427531837|\n",
      "| stddev|531.6553745894398|120.79440354929586|0.8953236140779876| 0.365187604471003|23.016842492553195|\n",
      "|    min|                0|               0.0|                 0|               0.0|               0.0|\n",
      "|    max|             3229|            8000.0|                12|              15.0|             531.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Outliers\n",
    "\n",
    "97% of our data falls between price range of 50 and 750, in order to get unbiased model, I will remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered = raw_data.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values\n",
    "\n",
    "Here, i did the following\n",
    "\n",
    "- Convert all the boolean variable to binary response (0,1)\n",
    "- Fill the missing values in the following columns with Zero\n",
    "    - security_deposit\n",
    "    - extra_people\n",
    "    - cleaning_fee\n",
    "    - All the review score columns\n",
    "    The assumption here is that: When these variable are missing, they are not applicable to the landloard\n",
    "\n",
    "- Calculation for Square feet missing values: The Assumption here is that When the square feet is less than or equals 100 and bedroom is 0, the square feet will be 350 and else the square feet will be 380 multiply by the number of bedrooms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.registerTempTable(\"data\")\n",
    "\n",
    "data_sql = spark.sql(\"\"\"\n",
    "    select\n",
    "        host_id,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        property_type,\n",
    "        case when host_is_superhost = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as host_is_superhost,\n",
    "        accommodates,\n",
    "        cancellation_policy,\n",
    "        minimum_nights,\n",
    "        maximum_nights,\n",
    "        availability_30,\n",
    "        availability_60,\n",
    "        availability_90,\n",
    "        availability_365,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        case when instant_bookable = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 0.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when review_scores_accuracy is null\n",
    "            then 0.0\n",
    "            else review_scores_accuracy\n",
    "        end as review_scores_accuracy,\n",
    "        case when review_scores_cleanliness is null\n",
    "            then 0.0\n",
    "            else review_scores_cleanliness\n",
    "        end as review_scores_cleanliness,\n",
    "        case when review_scores_checkin is null\n",
    "            then 0.0\n",
    "            else review_scores_checkin\n",
    "        end as review_scores_checkin,\n",
    "        case when review_scores_communication is null\n",
    "            then 0.0\n",
    "            else review_scores_communication\n",
    "        end as review_scores_communication,\n",
    "        case when review_scores_location is null\n",
    "            then 0.0\n",
    "            else review_scores_location\n",
    "        end as review_scores_location,\n",
    "        case when review_scores_value is null\n",
    "            then 0.0\n",
    "            else review_scores_value\n",
    "        end as review_scores_value,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet,\n",
    "        case when bathrooms >= 2\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as n_bathrooms_more_than_two,\n",
    "        case when amenity_wifi = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_wifi,\n",
    "        case when amenity_heating = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_heating,\n",
    "        case when amenity_essentials = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_essentials,\n",
    "        case when amenity_kitchen = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_kitchen,\n",
    "        case when amenity_tv = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_tv,\n",
    "        case when amenity_smoke_detector = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_smoke_detector,\n",
    "        case when amenity_washer = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_washer,\n",
    "        case when amenity_hangers = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_hangers,\n",
    "        case when amenity_laptop_friendly_workspace = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_laptop_friendly_workspace,\n",
    "        case when amenity_iron = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_iron,\n",
    "        case when amenity_shampoo = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_shampoo,\n",
    "        case when amenity_hair_dryer = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_hair_dryer,\n",
    "        case when amenity_family_kid_friendly = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_family_kid_friendly,\n",
    "        case when amenity_dryer = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_dryer,\n",
    "        case when amenity_fire_extinguisher = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_fire_extinguisher,\n",
    "        case when amenity_hot_water = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_hot_water,\n",
    "        case when amenity_internet = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_internet,\n",
    "        case when amenity_cable_tv = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_cable_tv,\n",
    "        case when amenity_carbon_monoxide_detector = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_carbon_monoxide_detector,\n",
    "        case when amenity_first_aid_kit = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_first_aid_kit,\n",
    "        case when amenity_host_greets_you = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_host_greets_you,\n",
    "        case when amenity_translation_missing_en_hosting_amenity_50 = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_translation_missing_en_hosting_amenity_50,\n",
    "        case when amenity_private_entrance = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_private_entrance,\n",
    "        case when amenity_bed_linens = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_bed_linens,\n",
    "        case when amenity_refrigerator = True\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as amenity_refrigerator\n",
    "    from data\n",
    "    where bedrooms is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sql = data_sql.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_sql.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make prediction, diffrent datatypes require different preprocessing techniques, so here, i get the column for:\n",
    "- Categorical Variables\n",
    "- Numberical Variable which consist of Double and Decimal datatypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [f.name for f in processed_data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "num_cols = [f.name for f in processed_data.schema.fields if isinstance(f.dataType, IntegerType)]\n",
    "decimal_cols = [f.name for f in processed_data.schema.fields if isinstance(f.dataType, DecimalType)]\n",
    "double_cols = [f.name for f in processed_data.schema.fields if isinstance(f.dataType, DoubleType)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = num_cols + decimal_cols + double_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Pipeline\n",
    "\n",
    "Here, i apply the following methods\n",
    "- String Indexer - This is used in Machine Learning algorithm to identify column as categorical variable, it converts the categorical column to numeric data and still keeping the categorical context.\n",
    "- One Hot Encodeing - This is a representation of categorical variables as binary vectors. The categorical values be mapped to integer values. \n",
    "- Vector Assembling - The last step in the Pipeline, this is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. You can do this by storing each of the values from a column as an entry in a vector. Then, from the model's point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "for x in cat_cols:\n",
    "    cat_string_indexer = StringIndexer(inputCol = x, outputCol = x + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[cat_string_indexer.getOutputCol()], outputCols=[x + \"encode\"])\n",
    "    stages += [cat_string_indexer, encoder]\n",
    "\n",
    "assembler_inputs = [c + \"encode\" for c in cat_cols] + num_features\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [assembler]                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Pipeline\n",
    "\n",
    "The Spark Pipeline is a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. \n",
    "\n",
    "My pipeline is in the following sequence StringIndexer > OneHotEncoderEstimator > VectorAssember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(processed_data)\n",
    "df = pipelineModel.transform(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "[training_data, validation_data] = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling \n",
    "\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'price', maxIter=10)\n",
    "model = gbt.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|price|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 78.12178682457298| 78.0|(87,[0,2,33,36,37...|\n",
      "| 98.58590164618633|100.0|(87,[0,2,33,36,37...|\n",
      "| 125.6894673684008|125.0|(87,[0,2,32,36,37...|\n",
      "| 78.28748129822688| 80.0|(87,[1,4,32,36,37...|\n",
      "| 74.41588097277969| 75.0|(87,[0,2,33,36,37...|\n",
      "|  97.2833287091994| 95.0|(87,[0,2,32,36,37...|\n",
      "| 98.58590164618633|100.0|(87,[0,2,32,36,37...|\n",
      "| 98.58590164618633|100.0|(87,[0,2,33,36,37...|\n",
      "|164.39502869446346|160.0|(87,[0,8,33,36,37...|\n",
      "|118.88180112091761|120.0|(87,[1,2,32,36,37...|\n",
      "|227.40137868950112|225.0|(87,[0,2,33,36,37...|\n",
      "| 64.41760033855675| 65.0|(87,[1,2,33,36,37...|\n",
      "|130.34811644810253|130.0|(87,[0,2,32,36,37...|\n",
      "|118.88180112091761|120.0|(87,[0,2,32,36,37...|\n",
      "| 87.14351857949916| 85.0|(87,[0,2,33,36,37...|\n",
      "|150.79300417376956|149.0|(87,[0,2,33,36,37...|\n",
      "|194.87018811810051|199.0|(87,[0,2,32,36,37...|\n",
      "|118.88180112091761|115.0|(87,[0,2,33,36,37...|\n",
      "|150.79300417376956|149.0|(87,[1,5,32,36,37...|\n",
      "| 152.1769859899222|159.0|(87,[1,5,32,36,37...|\n",
      "|150.79300417376956|149.0|(87,[1,2,32,36,37...|\n",
      "| 252.9108088948277|250.0|(87,[0,2,32,36,37...|\n",
      "|130.34811644810253|130.0|(87,[0,2,32,36,37...|\n",
      "| 74.41588097277969| 70.0|(87,[0,2,33,36,37...|\n",
      "| 52.92901442340954| 55.0|(87,[1,5,32,36,37...|\n",
      "|225.23124970544004|230.0|(87,[0,4,32,36,37...|\n",
      "|108.71416827067607|110.0|(87,[0,2,32,36,37...|\n",
      "|150.79300417376956|150.0|(87,[0,2,34,36,37...|\n",
      "| 74.31116997749825| 75.0|(87,[1,2,32,36,37...|\n",
      "|181.67994085495377|185.0|(87,[0,2,32,36,37...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt_predictions = model.transform(validation_data)\n",
    "gbt_predictions.select('prediction', 'price', 'features').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"model\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 13.8992\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Mean Squared Error (RMSE) on validation data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tunning the parameters, the best RMSE i got was 13.8992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the readme for the steps in building the API and server for scalability and reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
